{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, Dropout, LSTM, Softmax, Bidirectional, LayerNormalization, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/word_embed/train\"\n",
    "valid_path = \"data/word_embed/valid\"\n",
    "test_path = \"data/word_embed/test\"\n",
    "\n",
    "train_df = pd.DataFrame(columns=['text', 'stars', 'word_embed'])\n",
    "valid_df = pd.DataFrame(columns=['text', 'stars', 'word_embed'])\n",
    "test_df = pd.DataFrame(columns=['text', 'word_embed'])\n",
    "\n",
    "# load train data\n",
    "print(\"loading train data...\")\n",
    "for file in os.listdir(train_path):\n",
    "    df = pd.read_pickle(os.path.join(train_path, file))\n",
    "    train_df = train_df.append(df, ignore_index=True)\n",
    "    \n",
    "# load valid data\n",
    "print(\"loading valid data...\")\n",
    "for file in os.listdir(valid_path):\n",
    "    df = pd.read_pickle(os.path.join(valid_path, file))\n",
    "    valid_df = valid_df.append(df, ignore_index=True)\n",
    "\n",
    "# load test data\n",
    "print(\"loading test data...\")\n",
    "for file in os.listdir(test_path):\n",
    "    df = pd.read_pickle(os.path.join(test_path, file))\n",
    "    test_df = test_df.append(df, ignore_index=True)\n",
    "\n",
    "# convert to numpy array\n",
    "train_X = np.array(train_df['word_embed'].tolist())\n",
    "train_y = np.array(train_df['stars'].tolist())\n",
    "train_y = tf.keras.utils.to_categorical(train_y-1, num_classes = 5)\n",
    "\n",
    "valid_X = np.array(valid_df['word_embed'].tolist())\n",
    "valid_y = np.array(valid_df['stars'].tolist())\n",
    "valid_y = tf.keras.utils.to_categorical(valid_y-1, num_classes = 5)\n",
    "\n",
    "test_X = np.array(test_df['word_embed'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_X.shape=}\")\n",
    "print(f\"{train_y.shape=}\")\n",
    "print(f\"{valid_X.shape=}\")\n",
    "print(f\"{valid_y.shape=}\")\n",
    "print(f\"{test_X.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.argmax(train_y, axis=1), bins=5, color='blue', alpha=0.5, label='train')\n",
    "plt.hist(np.argmax(valid_y, axis=1), bins=5, color='red', alpha=0.5, label='valid')\n",
    "plt.title('Distribution of Stars')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, activation='relu'), input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_THRESHOLD = 0.99\n",
    "class accuryThreasholdCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('accuracy') > ACCURACY_THRESHOLD and logs.get('val_accuracy') > ACCURACY_THRESHOLD):   \n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n",
    "            self.model.stop_training = True\n",
    "\n",
    "accuracy_threashold_monitor = accuryThreasholdCallback()\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=15, batch_size=32, validation_data=(valid_X, valid_y), callbacks=[accuracy_threashold_monitor, early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'val'], loc='upper right')\n",
    "fig.suptitle('Model training history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_y = model.predict(valid_X)\n",
    "\n",
    "y_test_not_onehot = np.argmax(valid_y, axis=1)\n",
    "y_pred_not_onehot = np.argmax(valid_pred_y, axis=1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "matrix_confusion = confusion_matrix(y_pred_not_onehot, y_test_not_onehot)\n",
    "sns.heatmap(matrix_confusion, square=True, annot=False, cmap='Blues', fmt='d')\n",
    "\n",
    "for i in range(matrix_confusion.shape[0]):\n",
    "    for j in range(matrix_confusion.shape[1]):\n",
    "        plt.text(j+0.5, i+0.5, f'{matrix_confusion[i, j]}/{np.sum(matrix_confusion[i, :])}', \n",
    "                 horizontalalignment='center', verticalalignment='center', fontsize=7)\n",
    "\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_numerical = np.argmax(valid_y, axis=1)\n",
    "valid_pred_y_numerical = np.argmax(valid_pred_y, axis=1)\n",
    "\n",
    "print(classification_report(valid_y_numerical, valid_pred_y_numerical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "print(\"@@@@@@ model prediction on training set @@@@@@\")\n",
    "index  = np.random.choice(train_X.shape[0])\n",
    "\n",
    "print(f\"text: {train_text[index]}\")\n",
    "\n",
    "train_stars = np.argmax(train_y[index], axis=0) + 1\n",
    "print(f\"ground truth: {train_stars}\")\n",
    "\n",
    "train_stars_pred = model.predict(train_X[index].reshape(1, train_X[index].shape[0], train_X[index].shape[1]), verbose=0)\n",
    "train_stars_pred = np.argmax(train_stars_pred) + 1\n",
    "print(f\"model pred: {train_stars_pred}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "# testing\n",
    "print(\"@@@@@@ model prediction on testing set @@@@@@\")\n",
    "index = np.random.choice(test_X.shape[0])\n",
    "\n",
    "print(f\"text: {test_text[index]}\")\n",
    "\n",
    "test_stars_pred = model.predict(test_X[index].reshape(1, test_X[index].shape[0], test_X[index].shape[1]), verbose=0)\n",
    "test_stars_pred = np.argmax(test_stars_pred) + 1\n",
    "print(f\"model pred: {test_stars_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f\"weights/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = model.predict(valid_X, verbose=1)\n",
    "test_pred = model.predict(test_X, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_df = pd.DataFrame(valid_df)\n",
    "valid_pred_df['stars'] = np.argmax(valid_pred, axis=1) + 1\n",
    "print(valid_pred_df[[\"text\", \"stars\"]].head())\n",
    "\n",
    "test_pred_df = pd.DataFrame(test_df)\n",
    "test_pred_df['stars'] = np.argmax(test_pred, axis=1) + 1\n",
    "print(test_pred_df[[\"review_id\", \"text\", \"stars\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_df[[\"text\", \"stars\"]].to_csv(\"data/valid_pred.csv\", index=False)\n",
    "test_df[[\"review_id\", \"text\", \"stars\"]].to_csv(\"data/pred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5688be1e860b42f6f5c12c91b9e760acd4617dc2b5a7f23aa820cef89e5a00a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
